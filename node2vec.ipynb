{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c95cfb-9f25-48d8-a3fa-8756184a8557",
   "metadata": {},
   "source": [
    "## DGL中的DeepWalk和Node2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a53e77-19d7-4af3-baca-b44f563eb52b",
   "metadata": {},
   "source": [
    "这里我们讲解DGL里实现DeepWalk和Node2Vec的方法。值得注意的是，DeepWalk可以看作是参数p和参数q为1的Node2Vec模型。\n",
    "\n",
    "这里按照图嵌入通用框架的四个部分，信息提取器、映射函数、重构器和优化目标，来实现Node2Vec。\n",
    "\n",
    "具体地，信息提取器就是提取随机游走，映射函数就是一层Embedding Layer，重构器和优化目标即是由节点嵌入的内积构成的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b350a61-b4c9-4ed1-9299-bae85d0b6bfc",
   "metadata": {},
   "source": [
    "下面这部分代码来自DGL样例中的[node2vec.py](https://github.com/dmlc/dgl/blob/master/examples/pytorch/node2vec/model.py)，这里做了一点小修改来让它更简洁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c1957d-3105-47c4-980b-9dc40abbd6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/dgl/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl.sampling import node2vec_random_walk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4715e96e-3b94-4d1d-8921-39c04242ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node2Vec(nn.Module):\n",
    "\n",
    "    ################### PART 1 ###################\n",
    "    def __init__(self, g, embedding_dim, walk_length, p, q, num_walks=10, window_size=5, num_negatives=5,\n",
    "                 use_sparse=True, weight_name=None):\n",
    "        super(Node2Vec, self).__init__()\n",
    "\n",
    "        assert walk_length >= window_size\n",
    "\n",
    "        self.g = g\n",
    "        \n",
    "        # 一些基本的超参数\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.walk_length = walk_length\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.num_walks = num_walks\n",
    "        self.window_size = window_size\n",
    "        self.num_negatives = num_negatives\n",
    "        self.N = self.g.num_nodes()  # 节点数量\n",
    "        if weight_name is not None:\n",
    "            self.prob = weight_name\n",
    "        else:\n",
    "            self.prob = None\n",
    "\n",
    "        # 初始化映射函数里的参数\n",
    "        self.embedding = nn.Embedding(self.N, embedding_dim, sparse=use_sparse)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.embedding.reset_parameters()\n",
    "\n",
    "    def sample(self, batch):\n",
    "        \"\"\"\n",
    "        构建正和负样本。\n",
    "        正样本来自随机游走；\n",
    "        负样本来自随机采样。\n",
    "        \"\"\"\n",
    "        # batch其实就是一组节点\n",
    "        if not isinstance(batch, torch.Tensor):\n",
    "            batch = torch.tensor(batch)\n",
    "\n",
    "        batch = batch.repeat(self.num_walks)    # 直接复制节点，完成重复次数的计算\n",
    "        \n",
    "        # 采样正样本，使用DGL的random walk的方法\n",
    "        pos_traces = node2vec_random_walk(self.g, batch, self.p, self.q, self.walk_length, self.prob)\n",
    "        pos_traces = pos_traces.unfold(1, self.window_size, 1)    # 用窗口划分得到的随机游走\n",
    "        pos_traces = pos_traces.contiguous().view(-1, self.window_size)\n",
    "\n",
    "        # 采样负样本\n",
    "        neg_batch = batch.repeat(self.num_negatives)    # 直接复制节点，完成重复次数的计算\n",
    "        neg_traces = torch.randint(self.N, (neg_batch.size(0), self.walk_length))    # 直接随机采样图中节点序列作为负样本\n",
    "        neg_traces = torch.cat([neg_batch.view(-1, 1), neg_traces], dim=-1)\n",
    "        neg_traces = neg_traces.unfold(1, self.window_size, 1)    # 用窗口划分得到的随机游走\n",
    "        neg_traces = neg_traces.contiguous().view(-1, self.window_size)\n",
    "\n",
    "        return pos_traces, neg_traces\n",
    "\n",
    "    def forward(self, nodes=None):\n",
    "        \"\"\"\n",
    "        返回输入节点的Embedding\n",
    "        \"\"\"\n",
    "        emb = self.embedding.weight\n",
    "        if nodes is None:\n",
    "            return emb\n",
    "        else:\n",
    "            return emb[nodes]\n",
    "\n",
    "    def loss(self, pos_trace, neg_trace):\n",
    "        \"\"\"根据正负样本计算损失函数\"\"\"\n",
    "        e = 1e-15  # 出现在下面的地方，用于防止torch.log()中的输入为0\n",
    "\n",
    "        # 正样本的损失函数\n",
    "        pos_start, pos_rest = pos_trace[:, 0], pos_trace[:, 1:].contiguous()\n",
    "        w_start = self.embedding(pos_start).unsqueeze(dim=1)\n",
    "        w_rest = self.embedding(pos_rest)\n",
    "        pos_out = (w_start * w_rest).sum(dim=-1).view(-1)  # 计算内积\n",
    "\n",
    "        # 负样本的损失函数\n",
    "        neg_start, neg_rest = neg_trace[:, 0], neg_trace[:, 1:].contiguous()\n",
    "\n",
    "        w_start = self.embedding(neg_start).unsqueeze(dim=1)\n",
    "        w_rest = self.embedding(neg_rest)\n",
    "        neg_out = (w_start * w_rest).sum(dim=-1).view(-1)  # 计算内积\n",
    "\n",
    "        # 计算损失值\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_out) + e).mean()  # 希望start, rest的嵌入的内积大\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid(neg_out) + e).mean()  # 希望start, rest的嵌入的内积小\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "    \n",
    "    def loader(self, batch_size):\n",
    "\n",
    "        return DataLoader(torch.arange(self.N), batch_size=batch_size, shuffle=True, collate_fn=self.sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400c8018-c56a-4ffb-8224-8ca305c5b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练Node2Vec模型\n",
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    total_loss = 0\n",
    "    for pos_traces, neg_traces in loader:\n",
    "        pos_traces, neg_traces = pos_traces.to(device), neg_traces.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_traces, neg_traces)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bed33c8-b3bb-4706-a299-b2cf82cadfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, g):\n",
    "    model.eval()\n",
    "    z = model()\n",
    "    z = z.detach().numpy() # detach()将tensor从计算图中脱离出来，numpy()把tensor转换成numpy.array格式\n",
    "    _, acc = evaluate_node_classification(z, g.ndata['label'], g.ndata['train_mask'], g.ndata['test_mask'], max_iter=150)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ffe657-60a1-4ee7-8932-1c08b887cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为衡量生成的图嵌入的质量，我们可以训练一个线性模型（比如逻辑回归模型）来预测节点的标签\n",
    "def evaluate_node_classification(embedding_matrix, labels, train_mask, \n",
    "                                 test_mask, normalize_embedding=True, max_iter=1000):\n",
    "\n",
    "    if normalize_embedding:\n",
    "        embedding_matrix = normalize(embedding_matrix)\n",
    "\n",
    "    features_train = embedding_matrix[train_mask]\n",
    "    features_test = embedding_matrix[test_mask]\n",
    "    labels_train = labels[train_mask]\n",
    "    labels_test = labels[test_mask]\n",
    "\n",
    "\n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter=max_iter, multi_class='auto')\n",
    "    clf.fit(features_train, labels_train)\n",
    "\n",
    "    preds = clf.predict(features_test)\n",
    "    test_acc = accuracy_score(labels_test, preds)\n",
    "    return preds, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06aa4688-29e8-4a6b-b15f-1a0477a4516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = CoraGraphDataset('./data') # 将数据保存在data文件夹下\n",
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab53b5dc-e178-4a1d-9033-471bc1020fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 7.9236, Acc: 0.1590\n",
      "Epoch: 02, Loss: 5.7525, Acc: 0.1970\n",
      "Epoch: 03, Loss: 4.5620, Acc: 0.2390\n",
      "Epoch: 04, Loss: 3.6935, Acc: 0.2760\n",
      "Epoch: 05, Loss: 3.0286, Acc: 0.3390\n",
      "Epoch: 06, Loss: 2.5149, Acc: 0.3840\n",
      "Epoch: 07, Loss: 2.1284, Acc: 0.4320\n",
      "Epoch: 08, Loss: 1.8396, Acc: 0.4620\n",
      "Epoch: 09, Loss: 1.6125, Acc: 0.5010\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Node2Vec(g, embedding_dim=128, walk_length=20,\n",
    "                 p=1, q=1, num_walks=10, window_size=10,\n",
    "                 num_negatives=5, use_sparse=True).to(device)\n",
    "\n",
    "loader = model.loader(batch_size=128) # 提取共现信息\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "\n",
    "for epoch in range(10): # 这里使用更大的epoch将提高性能，比如100\n",
    "    loss = train(model, loader, optimizer, device)\n",
    "    acc = test(model, g)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab3256-a2d7-4cfd-8624-2e38b31c1a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl:Python",
   "language": "python",
   "name": "conda-env-dgl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
